# ğŸ§  Post-Mortem: [TÃ­tulo do Incidente]
> **Objetivo:** Documentar e analisar um incidente de forma construtiva, sem atribuiÃ§Ã£o de culpa, identificando causas, impactos e aprendizados.

---

## ğŸ“… Resumo do Incidente
| Campo | DescriÃ§Ã£o |
|--------|------------|
| ID | INC-XXXX |
| Data/Hora | [Data e hora do inÃ­cio e tÃ©rmino] |
| DuraÃ§Ã£o | [Tempo total de impacto] |
| Severidade | [P1, P2, P3...] |
| ServiÃ§os Afetados | [Lista de serviÃ§os] |
| Impacto | [Breve descriÃ§Ã£o do impacto no negÃ³cio ou usuÃ¡rio] |

---

## ğŸ” DetecÃ§Ã£o
Descreva como o incidente foi identificado:
- Qual alerta disparou?  
- Qual mÃ©trica ou evento foi o gatilho?  
- Quanto tempo levou para detectar o problema (MTTD)?

**Exemplo:**
> Alerta "Erro 5xx > 5%" no serviÃ§o `checkout-service` detectado via Prometheus.  
> Tempo de detecÃ§Ã£o: 3 minutos apÃ³s inÃ­cio da falha.

---

## ğŸ§ª DiagnÃ³stico
Liste os principais sinais e descobertas que levaram Ã  causa raiz:
- Logs, mÃ©tricas, traces relevantes.  
- Passos realizados durante a investigaÃ§Ã£o.  
- Ferramentas usadas para diagnÃ³stico.

**Exemplo:**
> Logs indicaram `SQLTimeoutException`. MÃ©tricas de pool de conexÃ£o saturadas.  
> InvestigaÃ§Ã£o mostrou aumento de trÃ¡fego + limitaÃ§Ã£o de conexÃµes.

---

## ğŸ› ï¸ CorreÃ§Ã£o
Descreva o que foi feito para restaurar o serviÃ§o:
1. AÃ§Ãµes tomadas (manual ou automÃ¡tica).  
2. SequÃªncia das aÃ§Ãµes.  
3. ValidaÃ§Ã£o pÃ³s-correÃ§Ã£o (como foi confirmada a normalizaÃ§Ã£o).

**Exemplo:**
> Reiniciado deployment `checkout-service` e ajustado `maximum-pool-size` para 30.  
> Healthcheck OK e latÃªncia < 300ms apÃ³s correÃ§Ã£o.

---

## ğŸ’¡ Causa Raiz
Explique a origem do problema e os fatores que contribuÃ­ram:
- Falha tÃ©cnica, processo, arquitetura, ou humana.  
- DecisÃµes que agravaram ou atrasaram a resposta.  

**Exemplo:**
> Falha no dimensionamento do pool de conexÃµes do banco de dados.  
> Alerta inexistente para uso de conexÃµes acima de 90%.

---

## ğŸ” AÃ§Ãµes Corretivas e Preventivas
| Tipo | DescriÃ§Ã£o | ResponsÃ¡vel | Prazo |
|------|------------|--------------|-------|
| Preventiva | Criar alerta para uso de conexÃµes > 90%. | SRE Team | 10/11 |
| Evolutiva | Implementar autoscaling no DB. | DevOps | 20/11 |
| Educacional | Revisar prÃ¡ticas de pooling no onboarding. | Engenharia | 30/11 |

---

## ğŸ“š LiÃ§Ãµes Aprendidas
Liste aprendizados relevantes para o time e para a organizaÃ§Ã£o:
- Quais sinais poderiam ter sido percebidos antes?  
- Quais decisÃµes ajudaram na resoluÃ§Ã£o?  
- Que mudanÃ§as de processo ou automaÃ§Ã£o surgiram daqui?

> â€œCada incidente Ã© uma oportunidade para melhorar sistemas, pessoas e processos.â€

---

## ğŸ§° Melhorias Relacionadas
- Atualizar runbook relacionado: [link]  
- Criar nova mÃ©trica de saturaÃ§Ã£o no dashboard.  
- Incluir alerta de latÃªncia no Prometheus.  
- Revisar dependÃªncias crÃ­ticas no pipeline.

---

## ğŸ“Š MÃ©tricas
| Indicador | Valor | ObservaÃ§Ã£o |
|------------|--------|-------------|
| MTTD (Mean Time to Detect) | [minutos] | Tempo atÃ© detecÃ§Ã£o |
| MTTR (Mean Time to Recover) | [minutos] | Tempo atÃ© resoluÃ§Ã£o |
| Tempo de Resposta do Alerta | [minutos] | Entre alerta e aÃ§Ã£o |
| Impacto Estimado | [valor ou percentual] | Financeiro, usuÃ¡rios, etc. |

---

## ğŸ§  ReflexÃ£o Blameless
> O objetivo deste post-mortem nÃ£o Ã© identificar culpados, e sim compreender **por que o sistema permitiu que o erro acontecesse**.

Perguntas para reflexÃ£o:
- O que dificultou a detecÃ§Ã£o precoce?  
- As decisÃµes tomadas foram baseadas em dados ou suposiÃ§Ãµes?  
- Que tipo de automaÃ§Ã£o poderia ter prevenido ou acelerado a resposta?  
- Que aprendizado o time leva daqui?

---

## ğŸ¤– Uso de IA (opcional)
Se houver uso de ferramentas inteligentes no processo:
- IA correlacionou alertas ou logs?  
- Gerou o rascunho inicial do post-mortem?  
- Identificou incidentes semelhantes?  
- PropÃ´s aÃ§Ãµes corretivas?

**Exemplo:**
> IA analisou 3 mil linhas de logs e detectou padrÃ£o idÃªntico ao incidente de agosto, sugerindo mesma correÃ§Ã£o.

---

## ğŸ“¦ Metadados
```yaml
status: [draft|reviewed|approved]
created_at: [AAAA-MM-DD]
last_updated: [AAAA-MM-DD]
reviewed_by: [responsÃ¡vel]
shared_with: [times envolvidos]
tags:
  - post-mortem
  - blameless
  - incident
  - reliability
```

---

## ğŸ“š ReferÃªncias e Recursos
- Dashboards: [link para Grafana]  
- Logs: [link para Kibana]  
- Runbook relacionado: [link]  
- DocumentaÃ§Ã£o de arquitetura: [link interno]  
- Post-Mortems anteriores: [lista ou repositÃ³rio]  

---

## ğŸ‘¥ ResponsÃ¡veis
| FunÃ§Ã£o | Nome / Time | Contato |
|--------|--------------|----------|
| Autor | [Nome] | [Email ou Slack] |
| Revisor TÃ©cnico | [Nome] | [Email] |
| Acompanhamento de AÃ§Ãµes | [Nome] | [Email] |

---

## ğŸ’¬ Notas Finais
> Post-mortems sÃ£o instrumentos de aprendizado contÃ­nuo.  
> Devem ser revisados e compartilhados para fortalecer a confiabilidade coletiva.  
> VersÃ£o 1.0 â€” Atualizado em [Data].
